---
title: "ecommerce_review_sentiment"
output: rmarkdown::github_document
---
## Sentiment Analysis for E-commerce Reviews

Sentiment analysis in R is a powerful tool you can use when quantifying text. Reading and processing vast amounts of text is often not viable and machine learning methods can help cope with that. In this case, we will be working with a sample dataset containing reviews for clothing items sold in the e-commerce space. You can apply the same techniques in any industry. I usually apply this to our biannual Net Promoter Score (NPS) survey, when our 70K+ users are requested to answer how likely they are to recommend our software. 

First, we set up our working directory. I'm using a local machine but you can choose to pull data in any way you like.
```{r wd, include=FALSE}
setwd("C:/Users/oguner/Documents/ecommerce_review_sentiment/ecommerce_review_sentiment")
```
Next, let's import some R libraries.We'll use these libraries for different actions we'll take. 
``` {r libs}
library(NLP)
library(stringr)
library(tm)
library(wordcloud)
library(dplyr)
library(udpipe)
library(ggplot2)
library(tidytext)
```

Let's load our data and take a look at it:

```{r load}
review<-read.csv("ecommerce_review.csv")
review$text<-review$text
head(review)
```
This is great, but we're more interested in the actual text column:

```{r loadr}
head(review$text)
```

Looks like we have our text data. Let's move on to pre-processing.

## Pre-processing
One of the important parts of text pre-processing is getting rid of punctuation.
``` {r punct}
review$text <- lapply(review$text, function(x) {
  x = gsub('[[:punct:]]', '', x)})
```

This was an important step I missed when I initially took on a similar project. Before I merged negative words, phrases such as "not good" or "not great" were separated and therefore the counts for "good" and "great" would increase. In this case, they will appear as "notgood" or "notgreat". We can easily distinguish between positive and negative.

``` {r negs}
review$text <- lapply(review$text, function(x) {
  x= gsub('not ','not',x)})
```

The next two steps will include cleaning out spaces.This will prevent the machine reading the words differently in case spaces between them are not standardized.

``` {r spaces2}
review$text <- lapply(review$text, function(x) {
  x= gsub('  ',' ',x)})
```

Let's clean up triple spaces as well. I think double and triple will do it for now. People rarely put in more spaces between words.
``` {r spaces3}
review$text <- lapply(review$text, function(x) {
  x= gsub('   ',' ',x)})
```

Converting to lowercase is another important function. This way, any capitalized words will be treated the same as uncapitalized words as long as the exact same letters and letter orders are used.

```  {r lowerandstops}
review$text <- lapply(review$text, function(x) {
  x= str_to_lower(x)  
  x = removeWords(x, words = stopwords(kind = "en"))})
```
Now let's take a look at our data:

``` {r view}
head(review$text)
```
Looks like we cleaned up our text field nicely. Pre-processing ends here. Let's move onto transforming our data.


## Transformation
Next step is converting the raw text to a character. Next stages will include converting characters into corpus and matrix, which will form our dataframe. The end result we want to achieve is basically a heatmap (a dataframe) where each row is a review and each column is the each word that appears at least once in the entire raw text pool.

Let's convert our field to characters:

``` {r char}
review$text<-as.character(review$text)
```

Converting characters to corpus...
``` {r corpus}
review_corpus <- VCorpus(VectorSource(review$text))
```

We'll create a document term matrix from our corpus:

``` {r dtm}
review_dtm <- DocumentTermMatrix(review_corpus)
```

In this step, we'll removesparse term. Certain terms from the dataset are removed which has at least a 'sparse' percentage of empty (i.e., terms occurring 0 times in a document) according to the tm documentation.

``` {r sparse}
review_dtm <- removeSparseTerms(review_dtm, 0.99)
```

Last step of the data transformation is converting the document-term matrics into a dataframe to achieve the desired solution of a heatmap of each words and frequency with which they are used in each row (i.e. review).

``` {r df}
DTM_df <- as.data.frame(as.matrix(review_dtm))
```

#Creating word counts for each row...

``` {r wordcounts}
wordcounts <- colSums(DTM_df)
```

Labeling word counts with the corresponding word's name...

``` {r wordNames} 
wordNames <- names(DTM_df)
```
Let's look at what our final form looks like:

``` {r df View}
head(DTM_df)
```

We achieved our goal of creating a heatmap where each row is a review and each column is a word that appears at least once in the entire dataset. This was the last stage in data transformation. We can move on to the visualization part.

## Visualization

Let's start by a simple bar chart

```{r barchart}
wordcount_barplot <- tibble(line = 1:nrow(review), text = review$text) %>%
    unnest_tokens(word, text) %>%
    anti_join(stop_words) %>%
    count(word, sort = TRUE)
barplot(height=head(wordcount_barplot,10)$n, names.arg=head(wordcount_barplot,10)$word, xlab="Words", ylab="Frequency", col="tomato1", main="Word frequency in reviews")
```

I'm not a fan of bar charts in natural language processing. The n you are able to discern with a naked eye is extremely low and it doesn't tell a story. I've found a that a word cloud is infinitely more impactful with business stakeholders if used correctly. Let's create a word cloud with the output data we had in our previous chunk:

``` {r wordcloud1}
wordcloud(c(wordNames), c(wordcounts), scale=c(4,.5), max.words=Inf, rot.per=.1, colors = brewer.pal(4, "Dark2"), ordered.colors=FALSE)
```

There are immediately a few issues with this: We can see that irrelevant words such as "and", "but", "this" are visible with big letters (which correspond to frequency). Truth be told, no one cares about these words. Depending on the context, you might want to keep some of these stopwords. For instance, a stakeholder wanted to see how many times the word "very" used in different contexts to determine the strength of adjectives used. That didn't lead to anywhere in our project but you might want to reconsider the stopwords you're trying to remove. So we'll add the code right below to remove the most frequent stopwords used in English language and re-run our code.

``` {r fix stopwords}
review$text <- lapply(review$text, function(x) {
  x = removeWords(x, words = stopwords(kind = "en"))})
```

Now let's take a look:

``` {r newcloud}
wordcloud(c(wordNames), c(wordcounts), scale=c(4,.5), max.words=200, rot.per=.1, colors = brewer.pal(4, "Dark2"), ordered.colors=FALSE)
```

Looks like the words "dress", "love", and "size" are teh 2 most commonly used words, in that order. Next comes "like", "just", "top", "wear", and "fit'. Let's make our job easier and get word counts in a table:

``` {r wordtable}
tibble(wordNames, wordcounts) %>% arrange(desc(wordcounts)) %>% top_n(15)
```

Let's put these results in a dataframe to be used later:

``` {r dataframe}
wordcounts <- as.data.frame(wordcounts)
wordNames <- as.data.frame(wordNames)
```
##End
